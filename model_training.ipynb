{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7207b66d",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This script is used for training the LightGBM model. All feature engineering was done previously in SAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d173aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c25c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ignore the annoying copy warnings from pandas\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a2a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File paths\n",
    "'''\n",
    "folder = os.path.join(\"/projectnb\",\"cs542sp\",\"netflix_wrw2\", \"CS542-final-project\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44b1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "Process the raw data into trainable data for the model\n",
    " - creates a train and validation set\n",
    "'''\n",
    "def process_data(data):\n",
    "    \n",
    "    data = data.dropna()\n",
    "    \n",
    "    for c in data.columns:\n",
    "        data.loc[:,c] = pd.to_numeric(data.loc[:,c], downcast=\"unsigned\")\n",
    "\n",
    "    data.info()\n",
    "    \n",
    "    X = data.drop(['User_ID','Movie_ID', 'Rated'], axis=1)\n",
    "    Y = data.loc[:,\"Rated\"]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e153e75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49371488 entries, 0 to 49999999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   User_ID                 uint32 \n",
      " 1   Movie_ID                uint16 \n",
      " 2   Year                    uint16 \n",
      " 3   Rated                   uint8  \n",
      " 4   Ratings_for_Movie       uint32 \n",
      " 5   Ratings_for_Movie_2005  uint32 \n",
      " 6   Netflix_Release_Year    uint16 \n",
      " 7   Movie_Rating_Time       float64\n",
      " 8   Movie_Ratings_per_Day   float64\n",
      " 9   Release_Year            uint16 \n",
      " 10  AVG_Rating_for_Movie    float64\n",
      " 11  Ratings_from_User       uint16 \n",
      " 12  Ratings_from_User_2005  uint16 \n",
      " 13  AVG_Rating_from_User    float64\n",
      " 14  User_Rating_Time        float64\n",
      " 15  User_Ratings_per_Day    float64\n",
      " 16  User_Entry_Year         uint16 \n",
      "dtypes: float64(6), uint16(7), uint32(3), uint8(1)\n",
      "memory usage: 3.8 GB\n",
      "CPU times: user 3min 23s, sys: 1min 50s, total: 5min 13s\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Read 50M data instances and process into useable data\n",
    "'''\n",
    "\n",
    "reader = pd.read_sas('netflix_analysis_dataset2.sas7bdat', chunksize=50_000_000)\n",
    "data = next(reader)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753c7a3",
   "metadata": {},
   "source": [
    "### Note the number of threads should be set to the number of CPUs available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0816e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of threads to use in training\n",
    "num_threads = 8\n",
    "\n",
    "# set the number of training rounds\n",
    "num_rounds = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccfdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.639096\n",
      "[20]\tvalid_0's binary_logloss: 0.630269\n",
      "[30]\tvalid_0's binary_logloss: 0.626752\n",
      "[40]\tvalid_0's binary_logloss: 0.624834\n",
      "[50]\tvalid_0's binary_logloss: 0.623482\n",
      "[60]\tvalid_0's binary_logloss: 0.622594\n",
      "[70]\tvalid_0's binary_logloss: 0.621729\n",
      "[80]\tvalid_0's binary_logloss: 0.621046\n",
      "[90]\tvalid_0's binary_logloss: 0.620506\n",
      "[100]\tvalid_0's binary_logloss: 0.619906\n",
      "[110]\tvalid_0's binary_logloss: 0.619277\n",
      "[120]\tvalid_0's binary_logloss: 0.6188\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Set the data\n",
    "'''\n",
    "train_data = lgbm.Dataset(X_train, label=y_train, free_raw_data = False)\n",
    "valid_set = lgbm.Dataset(X_valid, label=y_valid, reference=train_data, free_raw_data = False)\n",
    "\n",
    "'''\n",
    "Set the parameters\n",
    "'''\n",
    "params = {\n",
    "    \"objective\":'binary',\n",
    "    \"num_leaves\": 2047,\n",
    "    \"max_depth\": 12,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"bagging_fraction\": 0.7,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"metric\":\"binary_logloss\",\n",
    "    \"num_threads\": num_threads,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"min_data_in_leaf\":500,\n",
    "    \"verbose\":-1\n",
    "}\n",
    "\n",
    "\n",
    "model_evaluation = {}\n",
    "\n",
    "model = lgbm.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=num_rounds,\n",
    "    valid_sets=[valid_set],\n",
    "    early_stopping_rounds = 10,\n",
    "    verbose_eval = 10,\n",
    "    evals_result = model_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0e243",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot the feature importance of the model\n",
    "'''\n",
    "lgbm.plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f515d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "Evaluate the model train and valid set score\n",
    "'''\n",
    "def evaluate(model, X_train, X_valid, y_train, y_valid, num = 1_000_000):\n",
    "    \n",
    "    train_index = y_train.sample(num).index\n",
    "    test_index = y_valid.sample(num).index\n",
    "    \n",
    "    train_prediction = model.predict(X_train.loc[train_index])\n",
    "    valid_prediction = model.predict(X_valid.loc[test_index])\n",
    "    \n",
    "    train = accuracy_score(y_train.loc[train_index], train_prediction > 0.5)\n",
    "    test = accuracy_score(y_valid.loc[test_index], valid_prediction > 0.5)\n",
    "    \n",
    "    '''\n",
    "    Check for overfitting\n",
    "    '''\n",
    "    print('\\tTrain score:\\t{:.3f}'.format(train))\n",
    "    print('\\tTest score:\\t{:.3f}'.format(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4376b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(model,X_train, X_valid, y_train, y_valid, num=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb247b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics report from sklearn\n",
    "'''\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_index = y_valid.sample(100000).index\n",
    "\n",
    "valid_prediction = model.predict(X_valid.loc[test_index])\n",
    "\n",
    "print(classification_report(y_valid[test_index], valid_prediction > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8df90fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2b4e67822ac0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Save the model to the models folder\n",
    "'''\n",
    "model.save_model('models/m50M_500.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
